# -*- coding: utf-8 -*-
"""ADM final project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1asgPo0hO5cOwf8ZPFkwU8r1YxvkYVrCh
"""





from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, desc, to_date, split, explode

# Initialize the Spark session
spark = SparkSession.builder \
    .appName("User Behavior Profiling") \
    .getOrCreate()

# Load the datasets
logon_df = spark.read.csv('/content/drive/MyDrive/DataSets/logon_info.csv', header=True, inferSchema=True)
device_df = spark.read.csv('/content/drive/MyDrive/DataSets/device_info.csv', header=True, inferSchema=True)
http_df = spark.read.csv('/content/drive/MyDrive/DataSets/http_info.csv', header=True, inferSchema=True)

# Convert string date to date type (assumes ISO format; adjust format if necessary)
logon_df = logon_df.withColumn("date", to_date(col("date")))
device_df = device_df.withColumn("date", to_date(col("date")))
http_df = http_df.withColumn("date", to_date(col("date")))

# Ensuring date parsing went correctly
logon_df.show(5)
device_df.show(5)
http_df.show(5)

from pyspark.sql.functions import hour

# Extracting hour for analysis of logon time
logon_df = logon_df.withColumn("hour", hour("date"))

# Analyzing logon and logoff by hour
hourly_activity = logon_df.groupBy("hour").pivot("activity").count().na.fill(0)
hourly_activity.show()

# Analyze usage patterns of PCs
device_usage = device_df.groupBy("pc").agg(count("user").alias("usage_count")).orderBy(desc("usage_count"))
device_usage.show()

# Analyzing top websites visited
top_sites = http_df.groupBy("url").count().orderBy(desc("count")).limit(10)
top_sites.show()

# Analyzing frequent content keywords
content_keywords = http_df.withColumn("keyword", explode(split(col("content"), " "))).groupBy("keyword").count().orderBy(desc("count")).limit(10)
content_keywords.show()

from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, col

# Initialize Spark session
spark = SparkSession.builder \
    .appName("User Behavior Profiling") \
    .getOrCreate()

# Example creation of DataFrames (the following lines are placeholders and should be replaced with actual data loading and analysis code)
hourly_activity = spark.createDataFrame([(10, 100, 80), (11, 150, 90)], ["hour", "Logon", "Logoff"])
device_usage = spark.createDataFrame([("PC-001", 300), ("PC-002", 200)], ["pc", "usage_count"])
top_sites = spark.createDataFrame([("google.com", 500), ("example.com", 400)], ["url", "count"])
content_keywords = spark.createDataFrame([("keyword1", 1000), ("keyword2", 800)], ["keyword", "count"])

# Adding a 'report' column to each DataFrame that describes the type of data
hourly_activity = hourly_activity.withColumn("report", lit("Hourly Logon/Logoff"))
device_usage = device_usage.withColumn("report", lit("Device Usage"))
top_sites = top_sites.withColumn("report", lit("Top Visited Sites"))
content_keywords = content_keywords.withColumn("report", lit("Top Keywords"))

# Combining all DataFrames into a single DataFrame for a unified report
report = hourly_activity.select("report", "hour", col("Logon").alias("value1"), col("Logoff").alias("value2")) \
    .unionByName(device_usage.select("report", col("pc").alias("hour"), col("usage_count").alias("value1"), lit(None).alias("value2"))) \
    .unionByName(top_sites.select("report", col("url").alias("hour"), col("count").alias("value1"), lit(None).alias("value2"))) \
    .unionByName(content_keywords.select("report", col("keyword").alias("hour"), col("count").alias("value1"), lit(None).alias("value2")))

# Display the combined report
report.show(100, truncate=False)

# Stop Spark session when done
#spark.stop()

# Assuming the previous Spark DataFrames: hourly_activity, device_usage, top_sites, content_keywords
hourly_activity_pd = hourly_activity.toPandas()
device_usage_pd = device_usage.toPandas()
top_sites_pd = top_sites.toPandas()
content_keywords_pd = content_keywords.toPandas()

pip install matplotlib seaborn pandas

import matplotlib.pyplot as plt
import seaborn as sns

# Set the visualization style
sns.set(style="whitegrid")

# Visualizing Logon and Logoff activities by hour
plt.figure(figsize=(12, 6))
sns.barplot(data=hourly_activity_pd, x='hour', y='Logon', color='green', label='Logon')
sns.barplot(data=hourly_activity_pd, x='hour', y='Logoff', color='red', label='Logoff')
plt.title('Hourly Logon and Logoff Activities')
plt.xlabel('Hour of Day')
plt.ylabel('Count')
plt.legend()
plt.show()

# Visualizing Device Usage
plt.figure(figsize=(12, 6))
sns.barplot(data=device_usage_pd.head(10), x='pc', y='usage_count')
plt.title('Top 10 PCs by Usage Count')
plt.xlabel('PC')
plt.ylabel('Usage Count')
plt.xticks(rotation=45)
plt.show()

# Visualizing Top Visited Websites
plt.figure(figsize=(12, 6))
sns.barplot(data=top_sites_pd, x='url', y='count')
plt.title('Top Visited Websites')
plt.xlabel('Website')
plt.ylabel('Visit Count')
plt.xticks(rotation=90)
plt.show()

# Visualizing Top Content Keywords
plt.figure(figsize=(12, 6))
sns.barplot(data=content_keywords_pd, x='keyword', y='count')
plt.title('Top Content Keywords in Web Browsing')
plt.xlabel('Keyword')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

# Stop Spark session when all operations are done
spark.stop()





import pandas as pd

# Load the datasets
device_info_df = pd.read_csv('/content/drive/MyDrive/DataSets/device_info.csv')
file_info_df = pd.read_csv('/content/drive/MyDrive/DataSets/file_info.csv')
logon_info_df = pd.read_csv('/content/drive/MyDrive/DataSets/logon_info.csv')

# Display the first few rows of the dataset
print(device_info_df.head())
print(file_info_df.head())
print(logon_info_df.head())

# Explore datasets
print(device_info_df.info())
print(file_info_df.info())
print(logon_info_df.info())

import pandas as pd

# Load datasets
device_info = pd.read_csv('/content/drive/MyDrive/DataSets/device_info.csv')
file_info = pd.read_csv('/content/drive/MyDrive/DataSets/file_info.csv')
logon_info = pd.read_csv('/content/drive/MyDrive/DataSets/logon_info.csv')

# Convert 'date' columns to datetime
device_info['date'] = pd.to_datetime(device_info['date'])
file_info['date'] = pd.to_datetime(file_info['date'])
logon_info['date'] = pd.to_datetime(logon_info['date'])

# Assuming you want to analyze daily activity, extract the date without time
device_info['date_only'] = device_info['date'].dt.date
file_info['date_only'] = file_info['date'].dt.date
logon_info['date_only'] = logon_info['date'].dt.date

# Merge the dataframes on 'user' and 'date_only'
from functools import reduce
dfs = [device_info, file_info, logon_info]
combined_df = reduce(lambda left, right: pd.merge(left, right, on=['user', 'date_only'], how='outer'), dfs)

# Create some features: counts of different activities
combined_df['logon_count'] = combined_df.groupby(['user', 'date_only'])['activity_x'].transform('count')
combined_df['file_access_count'] = combined_df.groupby(['user', 'date_only'])['filename'].transform('count')
combined_df['device_activity_count'] = combined_df.groupby(['user', 'date_only'])['activity_y'].transform('count')

from sklearn.preprocessing import StandardScaler

# Features to be used in clustering
features = ['logon_count', 'file_access_count', 'device_activity_count']
scaler = StandardScaler()
combined_df[features] = scaler.fit_transform(combined_df[features])

from sklearn.cluster import KMeans

# Apply K-means clustering
kmeans = KMeans(n_clusters=5, random_state=42)
combined_df['cluster'] = kmeans.fit_predict(combined_df[features])

# Print cluster centers
print(kmeans.cluster_centers_)

# Analyze user distribution across clusters
cluster_distribution = combined_df['cluster'].value_counts()
print(cluster_distribution)

# Detailed analysis of each cluster
for i in range(5):
    print(f"\nCluster {i} details:")
    display(combined_df[combined_df['cluster'] == i].describe())





import pandas as pd

# Assuming 'combined_df' is your DataFrame after clustering and it contains a 'date' column
combined_df['date'] = pd.to_datetime(combined_df['date'])

# Extract day of the week and hour of day
combined_df['day_of_week'] = combined_df['date'].dt.dayofweek
combined_df['hour_of_day'] = combined_df['date'].dt.hour

print(combined_df.columns)

# Assuming 'pc' represents the device adequately for all entries
user_profile = combined_df.groupby(['user', 'day_of_week', 'hour_of_day', 'pc']).size().reset_index(name='frequency')

# Get the most common cluster for each combination
most_common_clusters = combined_df.groupby(['user', 'day_of_week', 'hour_of_day', 'pc'])['cluster'].agg(lambda x: x.mode()[0]).reset_index(name='most_common_cluster')

# Merge this back with the user_profile DataFrame
user_profile = user_profile.merge(most_common_clusters, on=['user', 'day_of_week', 'hour_of_day', 'pc'], how='left')

# Display the first few rows of the resulting DataFrame to verify the results
print(user_profile.head())



#recomender system

!pip install implicit

import pandas as pd

# Load the dataset
file_path = '/content/drive/MyDrive/DataSets/file_info.csv'
data = pd.read_csv(file_path)

# Print the column names to inspect them
print(data.columns)

!pip install pyspark

from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("ALS Recommendation Example") \
    .getOrCreate()

# Load the CSV file into a DataFrame
file_df = spark.read.csv("/content/drive/MyDrive/DataSets/file_info.csv", header=True, inferSchema=True)

# Show the schema and first few rows of the DataFrame
file_df.printSchema()
file_df.show(5)

# Now you can use the ALS model training and recommendation code with this DataFrame

from pyspark.sql.functions import when, col

# Create a binary rating column
file_df_with_rating = file_df.withColumn("rating", when(col("content").isNull(), 0).otherwise(1))

# Convert string columns to index
indexers = [StringIndexer(inputCol=col, outputCol=col+"_index").fit(file_df_with_rating) for col in ["user", "filename"]]

pipeline = Pipeline(stages=indexers)
file_df_indexed = pipeline.fit(file_df_with_rating).transform(file_df_with_rating)

# Configure and train the ALS model
als = ALS(maxIter=5,
          regParam=0.01,
          userCol="user_index",
          itemCol="filename_index",
          ratingCol="rating",  # Use the newly created rating column
          coldStartStrategy="drop")
model = als.fit(file_df_indexed)

# Make predictions
predictions = model.transform(file_df_indexed)
predictions.show()

# Generate top 5 recommendations for each user
user_recs = model.recommendForAllUsers(5)
user_recs.show(truncate=False)

# Optionally, to show recommendations for a specific user, assuming you know their userIndex
# Let's say you want recommendations for userIndex = 3
specific_user_recs = user_recs.filter(user_recs.user_index == 3)
specific_user_recs.show(truncate=False)