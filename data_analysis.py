# -*- coding: utf-8 -*-
"""ADM Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YIccg2nhRqNvVnX_EEFc3MhMfd3uP0Fm
"""

4pip install pyspark

#logon dataset

from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Logon Info Analysis") \
    .getOrCreate()

logon_info_df = spark.read.option("header", "true").csv("/content/drive/MyDrive/DataSets/logon_info.csv")

logon_info_df.printSchema()
logon_info_df.show()

from pyspark.sql.functions import col, count, when

print("Missing values count:")
logon_info_df.select([count(when(col(c).isNull(), c)).alias(c) for c in logon_info_df.columns]).show()

from pyspark.sql.functions import to_timestamp

logon_info_df = logon_info_df.withColumn("date", to_timestamp("date", "MM/dd/yyyy HH:mm:ss"))

logon_info_df.printSchema()
logon_info_df.show()

print("Descriptive statistics:")
logon_info_df.describe().show()

print("Unique values in 'user' column:")
logon_info_df.select("user").distinct().show()

print("Unique values in 'pc' column:")
logon_info_df.select("pc").distinct().show()

print("Unique values in 'activity' column:")
logon_info_df.select("activity").distinct().show()

# Separate Logon and Logoff Records
logon_df = logon_info_df.filter(col("activity") == "Logon")
logoff_df = logon_info_df.filter(col("activity") == "Logoff")

# Display the counts of Logon and Logoff records
print("Logon records count:", logon_df.count())
print("Logoff records count:", logoff_df.count())

# Analyze Logon Activities
print("Unique users in Logon records:")
logon_df.select("user").distinct().show()

print("Unique PCs in Logon records:")
logon_df.select("pc").distinct().show()

# Analyze Logoff Activities
print("Unique users in Logoff records:")
logoff_df.select("user").distinct().show()

print("Unique PCs in Logoff records:")
logoff_df.select("pc").distinct().show()

import matplotlib.pyplot as plt

# Define data for visualization
activities = ['Logon', 'Logoff']
counts = [logon_df.count(), logoff_df.count()]

# Create a bar plot
plt.figure(figsize=(8, 6))
plt.bar(activities, counts, color=['blue', 'green'])
plt.xlabel('Activity')
plt.ylabel('Count')
plt.title('Count of Logon and Logoff Activities')
plt.show()

spark.stop()



# device dataset

from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Device_Data_Analysis") \
    .getOrCreate()

# Load the device dataset
device_df = spark.read.csv("/content/drive/MyDrive/DataSets/device_info.csv", header=True, inferSchema=True)

# Display the schema
print("Device DataFrame Schema:")
device_df.printSchema()

# Show the first few rows of the DataFrame
print("Sample records from device DataFrame:")
device_df.show(5)

from pyspark.sql.functions import col, count, when

# Check for missing values
print("Missing values in device DataFrame:")
device_df.select([count(when(col(c).isNull(), c)).alias(c) for c in device_df.columns]).show()

# Count occurrences of each activity type
activity_counts = device_df.groupBy('activity').count().orderBy('count', ascending=False)

# Show the results
print("Activity counts in device DataFrame:")
activity_counts.show()

# Count unique users
unique_users = device_df.select('user').distinct().count()
print("Number of unique users in device DataFrame:", unique_users)

# Count unique PCs
unique_pcs = device_df.select('pc').distinct().count()
print("Number of unique PCs in device DataFrame:", unique_pcs)

import matplotlib.pyplot as plt
import pandas as pd

# Convert Spark DataFrame to Pandas DataFrame
device_pd_df = device_df.toPandas()

# Plotting the distribution of activities (connect/disconnect)
activity_counts = device_pd_df['activity'].value_counts()
plt.figure(figsize=(8, 6))
activity_counts.plot(kind='bar', color='skyblue')
plt.title('Distribution of Activities')
plt.xlabel('Activity')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Plotting the trend of connections and disconnections over time
device_pd_df['date'] = pd.to_datetime(device_pd_df['date'])
device_pd_df.set_index('date', inplace=True)
activity_trend = device_pd_df.groupby('activity').resample('D').size().unstack(level=0).fillna(0)
activity_trend.plot(kind='line', figsize=(10, 6))
plt.title('Trend of Connections and Disconnections Over Time')
plt.xlabel('Date')
plt.ylabel('Count')
plt.legend(title='Activity')
plt.show()

# Plotting the top users with the most activities
top_users = device_pd_df['user'].value_counts().nlargest(10)
plt.figure(figsize=(10, 6))
top_users.plot(kind='bar', color='salmon')
plt.title('Top Users with the Most Activities')
plt.xlabel('User')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Plotting the top PCs with the most activities
top_pcs = device_pd_df['pc'].value_counts().nlargest(10)
plt.figure(figsize=(10, 6))
top_pcs.plot(kind='bar', color='lightgreen')
plt.title('Top PCs with the Most Activities')
plt.xlabel('PC')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

spark.stop()



#file dataset

from pyspark.sql import SparkSession

# Create a SparkSession
spark = SparkSession.builder \
    .appName("Logon Info Analysis") \
    .getOrCreate()

# Load the dataset into a Spark DataFrame
file_info_df = spark.read.option("header", "true").csv("/content/drive/MyDrive/DataSets/file_info.csv")

# Display the schema and first few rows of the DataFrame
file_info_df.printSchema()
file_info_df.show(5)

from pyspark.sql.functions import isnull, count, when

# Convert the 'date' column to datetime format
file_info_df = file_info_df.withColumn('date', to_timestamp(file_info_df['date'], 'MM/dd/yyyy HH:mm:ss'))

# Check for missing values
print("Missing values in file_info_df:")
file_info_df.select([count(when(isnull(c), c)).alias(c) for c in file_info_df.columns]).show()

import pyspark.sql.functions as F
import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Extract file extensions from the 'filename' column
file_info_df = file_info_df.withColumn('file_extension', F.substring_index('filename', '.', -1))

# Distribution of File Types
file_type_counts = file_info_df.groupBy('file_extension').count().orderBy('count', ascending=False).toPandas()

plt.figure(figsize=(10, 6))
plt.bar(file_type_counts['file_extension'], file_type_counts['count'], color='skyblue')
plt.xlabel('File Type')
plt.ylabel('Count')
plt.title('Distribution of File Types')
plt.xticks(rotation=45)
plt.show()

# Top Users with Most Files
top_users_files = file_info_df.groupBy('user').count().orderBy('count', ascending=False).limit(10).toPandas()

plt.figure(figsize=(10, 6))
plt.bar(top_users_files['user'], top_users_files['count'], color='salmon')
plt.xlabel('User')
plt.ylabel('Count')
plt.title('Top Users with Most Files')
plt.xticks(rotation=45)
plt.show()

# File Upload Trend Over Time
file_info_df.createOrReplaceTempView('file_info')
file_upload_trend = spark.sql("""
    SELECT DATE(date) AS upload_date, COUNT(*) AS num_files
    FROM file_info
    GROUP BY upload_date
    ORDER BY upload_date
""").toPandas()

plt.figure(figsize=(10, 6))
plt.plot(file_upload_trend['upload_date'], file_upload_trend['num_files'], marker='o', color='green')
plt.xlabel('Date')
plt.ylabel('Number of Files Uploaded')
plt.title('File Upload Trend Over Time')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

# Word Cloud of File Content
all_content = ' '.join(file_info_df.select('content').rdd.flatMap(lambda x: x).collect())

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_content)
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of File Content')
plt.show()

# Distribution of File Sizes
file_sizes = file_info_df.select(F.length('content').alias('file_size')).toPandas()

plt.figure(figsize=(10, 6))
plt.hist(file_sizes['file_size'], bins=30, color='orange', edgecolor='black')
plt.xlabel('File Size')
plt.ylabel('Frequency')
plt.title('Distribution of File Sizes')
plt.grid(True)
plt.show()

# Top PCs with Most Files
top_pcs_files = file_info_df.groupBy('pc').count().orderBy('count', ascending=False).limit(10).toPandas()

plt.figure(figsize=(10, 6))
plt.bar(top_pcs_files['pc'], top_pcs_files['count'], color='lightblue')
plt.xlabel('PC')
plt.ylabel('Count')
plt.title('Top PCs with Most Files')
plt.xticks(rotation=45)
plt.show()





# details dataset

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Data Analysis") \
    .getOrCreate()

# Load all CSV files into a single DataFrame
data_paths = [
    "/content/drive/MyDrive/DataSets/details/2010-01.csv",
    "/content/drive/MyDrive/DataSets/details/2010-02.csv",
    "/content/drive/MyDrive/DataSets/details/2010-03.csv",
    "/content/drive/MyDrive/DataSets/details/2010-04.csv",
    "/content/drive/MyDrive/DataSets/details/2010-05.csv",
    "/content/drive/MyDrive/DataSets/details/2010-06.csv",
    "/content/drive/MyDrive/DataSets/details/2010-07.csv",
    "/content/drive/MyDrive/DataSets/details/2010-08.csv",
    "/content/drive/MyDrive/DataSets/details/2010-09.csv",
    "/content/drive/MyDrive/DataSets/details/2010-10.csv",
    "/content/drive/MyDrive/DataSets/details/2010-11.csv",
    "/content/drive/MyDrive/DataSets/details/2010-12.csv"
]

combined_df = spark.read.option("header", "true").csv(data_paths)

# Display schema and first few rows of the combined DataFrame
combined_df.printSchema()
combined_df.show(5)

# Analysis: Distribution of employees across different roles
role_counts = combined_df.groupBy('role').count().orderBy('count', ascending=False)
print("Distribution of employees across different roles:")
role_counts.show()

# Analysis: Distribution of employees across different departments
dept_counts = combined_df.groupBy('department').count().orderBy('count', ascending=False)
print("Distribution of employees across different departments:")
dept_counts.show()

# Analysis: Count of employees supervised by each supervisor
supervisor_counts = combined_df.groupBy('supervisor').count().orderBy('count', ascending=False)
print("Count of employees supervised by each supervisor:")
supervisor_counts.show()

import matplotlib.pyplot as plt

# Visualization: Distribution of employees across different roles
role_counts_pd = role_counts.toPandas()
plt.figure(figsize=(10, 6))
plt.bar(role_counts_pd['role'], role_counts_pd['count'], color='skyblue')
plt.xlabel('Role')
plt.ylabel('Count')
plt.title('Distribution of Employees Across Different Roles')
plt.xticks(rotation=45, ha='right')
plt.show()

import matplotlib.pyplot as plt

# Drop rows with null values in the 'department' column
dept_counts_pd = dept_counts_pd.dropna(subset=['department'])

# Plotting the distribution of employees across different departments
plt.figure(figsize=(10, 6))
plt.bar(dept_counts_pd['department'], dept_counts_pd['count'], color='salmon')
plt.xlabel('Department')
plt.ylabel('Count')
plt.title('Distribution of Employees Across Different Departments')
plt.xticks(rotation=45, ha='right')
plt.show()

# Visualization: Count of employees supervised by each supervisor
supervisor_counts_pd = supervisor_counts.toPandas().head(10)
plt.figure(figsize=(12, 8))
plt.barh(supervisor_counts_pd['supervisor'], supervisor_counts_pd['count'], color='lightgreen')
plt.xlabel('Count')
plt.ylabel('Supervisor')
plt.title('Count of Employees Supervised by Each Supervisor')
plt.gca().invert_yaxis()
plt.show()

spark.stop()



from pyspark.sql import SparkSession
from pyspark.sql.functions import countDistinct

# Step 1: Initialize Spark session
spark = SparkSession.builder \
    .appName("Logon_Device_Analysis") \
    .getOrCreate()

# Step 2: Load logon.csv
logon_df = spark.read.csv("/content/drive/MyDrive/DataSets/logon_info.csv", header=True)

# Step 3: Load device.csv
device_df = spark.read.csv("/content/drive/MyDrive/DataSets/device_info.csv", header=True)

# Step 4: Display schema of both dataframes
print("Schema of logon_df:")
logon_df.printSchema()

print("\nSchema of device_df:")
device_df.printSchema()

# Step 6: Merge datasets based on common columns (e.g., user and pc)
combined_df = logon_df.join(device_df, ['user', 'pc'], 'inner')

# Step 7: Show sample records from the combined DataFrame
print("Sample records from the combined DataFrame:")
combined_df.show(10)

spark.stop()